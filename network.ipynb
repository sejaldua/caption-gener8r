{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests as rq\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.layers import (LSTM, Embedding, Input, Dense, Dropout)\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers.merge import add\n",
    "from random import randint, sample\n",
    "import csv\n",
    "from keras.callbacks import LambdaCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\tpreprocessed\n",
      "200\tpreprocessed\n",
      "300\tpreprocessed\n",
      "400\tpreprocessed\n",
      "500\tpreprocessed\n",
      "600\tpreprocessed\n",
      "700\tpreprocessed\n",
      "800\tpreprocessed\n",
      "900\tpreprocessed\n",
      "1000\tpreprocessed\n",
      "1100\tpreprocessed\n",
      "1200\tpreprocessed\n",
      "1300\tpreprocessed\n",
      "1400\tpreprocessed\n",
      "1500\tpreprocessed\n",
      "1600\tpreprocessed\n",
      "1700\tpreprocessed\n",
      "1800\tpreprocessed\n",
      "1900\tpreprocessed\n",
      "2000\tpreprocessed\n",
      "2100\tpreprocessed\n",
      "2200\tpreprocessed\n",
      "2300\tpreprocessed\n",
      "2400\tpreprocessed\n",
      "2500\tpreprocessed\n",
      "2600\tpreprocessed\n",
      "2700\tpreprocessed\n",
      "2800\tpreprocessed\n",
      "2900\tpreprocessed\n",
      "3000\tpreprocessed\n",
      "3100\tpreprocessed\n",
      "3200\tpreprocessed\n",
      "3300\tpreprocessed\n",
      "3400\tpreprocessed\n",
      "3500\tpreprocessed\n",
      "3600\tpreprocessed\n",
      "3700\tpreprocessed\n",
      "3800\tpreprocessed\n",
      "3900\tpreprocessed\n",
      "4000\tpreprocessed\n",
      "4100\tpreprocessed\n",
      "4200\tpreprocessed\n",
      "4300\tpreprocessed\n",
      "4400\tpreprocessed\n",
      "4500\tpreprocessed\n",
      "4600\tpreprocessed\n",
      "4700\tpreprocessed\n",
      "4800\tpreprocessed\n",
      "4900\tpreprocessed\n",
      "5000\tpreprocessed\n",
      "5100\tpreprocessed\n",
      "5200\tpreprocessed\n",
      "5300\tpreprocessed\n",
      "5400\tpreprocessed\n",
      "5500\tpreprocessed\n",
      "5600\tpreprocessed\n",
      "5700\tpreprocessed\n",
      "5800\tpreprocessed\n",
      "5900\tpreprocessed\n",
      "6000\tpreprocessed\n",
      "6100\tpreprocessed\n",
      "6200\tpreprocessed\n",
      "6300\tpreprocessed\n",
      "6400\tpreprocessed\n",
      "6500\tpreprocessed\n",
      "6600\tpreprocessed\n",
      "6700\tpreprocessed\n",
      "6800\tpreprocessed\n",
      "6900\tpreprocessed\n",
      "7000\tpreprocessed\n",
      "7100\tpreprocessed\n",
      "7200\tpreprocessed\n",
      "7300\tpreprocessed\n",
      "7400\tpreprocessed\n",
      "7500\tpreprocessed\n",
      "7600\tpreprocessed\n",
      "7700\tpreprocessed\n",
      "7800\tpreprocessed\n",
      "7900\tpreprocessed\n",
      "8000\tpreprocessed\n",
      "8100\tpreprocessed\n",
      "8200\tpreprocessed\n",
      "8300\tpreprocessed\n",
      "8400\tpreprocessed\n",
      "8500\tpreprocessed\n",
      "8600\tpreprocessed\n",
      "8700\tpreprocessed\n",
      "8800\tpreprocessed\n",
      "8900\tpreprocessed\n",
      "9000\tpreprocessed\n",
      "9100\tpreprocessed\n",
      "9200\tpreprocessed\n",
      "9300\tpreprocessed\n",
      "9400\tpreprocessed\n",
      "9500\tpreprocessed\n",
      "9600\tpreprocessed\n",
      "9700\tpreprocessed\n",
      "9800\tpreprocessed\n",
      "9900\tpreprocessed\n",
      "10000\tpreprocessed\n"
     ]
    }
   ],
   "source": [
    "# Model to pre-process images\n",
    "cnn_model = VGG16()\n",
    "\n",
    "# re-structure the model\n",
    "cnn_model.layers.pop()\n",
    "cnn_model = Model(inputs=cnn_model.inputs, outputs=cnn_model.layers[-1].output)\n",
    "\n",
    "# Read in twitter data [cols: images, captions]\n",
    "data = pd.read_csv(\"masterdata.csv\")\n",
    "l = len(data)\n",
    "\n",
    "# List to hold the images' feature vectors\n",
    "features = []\n",
    "\n",
    "# List to hold the captions\n",
    "captions = []\n",
    "\n",
    "# List to hold the indices explored\n",
    "indices = []\n",
    "\n",
    "# Dimension of images\n",
    "image_dim = 224\n",
    "\n",
    "# Number of image-caption pairs to extract\n",
    "num_images = 10000\n",
    "\n",
    "# Maximum number of characters that a caption can be\n",
    "longest_caption = 60\n",
    "\n",
    "while len(features) < num_images:\n",
    "    try:\n",
    "        i = randint(0, l)\n",
    "        if i in indices:\n",
    "            continue\n",
    "        elif len(data.caption[i]) > longest_caption:\n",
    "            continue\n",
    "        try:\n",
    "            # Get image from URL and run it through VGG16 to get feature vector\n",
    "            url = data.photo[i]\n",
    "            response = rq.get(url)\n",
    "            img = Image.open(BytesIO(response.content)).resize((image_dim,image_dim))\n",
    "            x = image.img_to_array(img)\n",
    "            x = x.reshape((1, x.shape[0], x.shape[1], x.shape[2]))\n",
    "            x = preprocess_input(x)\n",
    "            f = cnn_model.predict(x)\n",
    "            features.append(f)\n",
    "            \n",
    "            # Append caption\n",
    "            captions.append(data.caption[i])\n",
    "            \n",
    "            # Append index\n",
    "            indices.append(i)\n",
    "        except:\n",
    "            continue\n",
    "    except:\n",
    "        print(data.caption[i])\n",
    "        continue\n",
    "        \n",
    "    # Print statement to check in on progress\n",
    "    if (len(features) % 100 == 0):\n",
    "        print(str(len(features)) + \"\\tpreprocessed\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all newline characters in captions\n",
    "captions = [c.replace('\\n', ' ') for c in captions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize captions: leave punctuation and upper-case letters, tokenize on a char level\n",
    "tokenizer = Tokenizer(lower=False, char_level=True,filters='\\t\\n')\n",
    "tokenizer.fit_on_texts(captions)\n",
    "encoded_captions = tokenizer.texts_to_sequences(captions)\n",
    "start = len(tokenizer.word_index) + 1\n",
    "stop = start + 1\n",
    "vocab_size = stop + 1\n",
    "\n",
    "# Insert start and stop sequences to the encoded caption size\n",
    "encoded_captions = [([start] + c) for c in encoded_captions]\n",
    "encoded_captions = [(c + [stop]) for c in encoded_captions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write extracted data to file\n",
    "\n",
    "import csv\n",
    "\n",
    "with open(\"encoded.csv\",\"w\") as f:\n",
    "    wr = csv.writer(f)\n",
    "    wr.writerows(encoded_captions)\n",
    "    \n",
    "with open(\"captions.csv\",\"w\") as f:\n",
    "    wr = csv.writer(f)\n",
    "    wr.writerows(captions)\n",
    "\n",
    "with open(\"features.csv\",\"w\") as f:\n",
    "    wr = csv.writer(f)\n",
    "    wr.writerows(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input-output vectors\n",
    "# Input: image feature vector, first n characters in caption\n",
    "# Output: n+1 character\n",
    "\n",
    "max_cap = max(len(c) for c in encoded_captions)\n",
    "X1 = []\n",
    "X2 = []\n",
    "y = []\n",
    "\n",
    "for i in range(len(encoded_captions)):\n",
    "    c = encoded_captions[i]\n",
    "    for j in range(len(c)):\n",
    "        in_seq, out_seq = c[:j], c[j]\n",
    "        in_seq = pad_sequences([in_seq], max_cap)[0]\n",
    "        out_seq = to_categorical(out_seq, num_classes = vocab_size)\n",
    "        X1.append(features[i])\n",
    "        X2.append(in_seq)\n",
    "        y.append(out_seq)\n",
    "X1 = np.reshape(X1,(np.shape(X1)[0], np.shape(X1)[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/camillebowman/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py:3985: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 62)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 4096)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 62, 256)      269312      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 4096)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 62, 256)      0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          1048832     dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 256)          525312      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 256)          0           dense_1[0][0]                    \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1052)         270364      dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,179,612\n",
      "Trainable params: 2,179,612\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# feature extractor model\n",
    "inputs1 = Input(shape=(4096,))\n",
    "fe1 = Dropout(0.5)(inputs1)\n",
    "fe2 = Dense(256, activation='relu')(fe1)\n",
    "\n",
    "# sequence model\n",
    "inputs2 = Input(shape=(max_cap,))\n",
    "se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "se2 = Dropout(0.5)(se1)\n",
    "se3 = LSTM(256)(se2)\n",
    "\n",
    "# decoder model\n",
    "decoder1 = add([fe2, se3])\n",
    "decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\n",
    "# tie it together [image, seq] [word]\n",
    "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "# summarize model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert a token to a letter\n",
    "def to_letter(yhat):\n",
    "    for k, v in tokenizer.word_index.items():\n",
    "        if v == yhat:\n",
    "            return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Function to test model at the end of each epoch -- might need a little work\n",
    "def on_epoch_end(epoch, _):\n",
    "    \"\"\"Function invoked at end of each epoch. Prints generated text.\"\"\"\n",
    "    print()\n",
    "    print('Generating caption after epoch %d' % epoch)\n",
    "\n",
    "    i = randint(0, num_images)\n",
    "    generated = ''\n",
    "    print('-------- Real caption: \"' + captions[i] + '\"')\n",
    "    x_pred = np.zeros((1, max_cap))\n",
    "    x_pred[0] = start\n",
    "    f = np.array(features[i])\n",
    "    \n",
    "    for i in range(max_cap):\n",
    "        preds = np.argmax(model.predict([f, x_pred], verbose=0))\n",
    "        if preds == start or preds == stop:\n",
    "            break\n",
    "        next_char = to_letter(preds)\n",
    "        generated += next_char\n",
    "    print(\"--- Generated caption: \\\"\" + generated + \"\\\"\")\n",
    "    print()\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 243540 samples, validate on 60886 samples\n",
      "Epoch 1/15\n",
      "243540/243540 [==============================] - 3200s 13ms/step - loss: 1.8230 - accuracy: 0.5048 - val_loss: 2.1200 - val_accuracy: 0.4820\n",
      "\n",
      "Generating caption after epoch 0\n",
      "----- Real caption: \"Did I even wear heels?\"\n",
      "                                                              \n",
      "\n",
      "Epoch 2/15\n",
      "243540/243540 [==============================] - 2622s 11ms/step - loss: 1.8128 - accuracy: 0.5062 - val_loss: 2.1273 - val_accuracy: 0.4826\n",
      "\n",
      "Generating caption after epoch 1\n",
      "----- Real caption: \"üóù to my garden\"\n",
      "\n",
      "\n",
      "Epoch 3/15\n",
      "243540/243540 [==============================] - 2261s 9ms/step - loss: 1.8053 - accuracy: 0.5088 - val_loss: 2.1514 - val_accuracy: 0.4847\n",
      "\n",
      "Generating caption after epoch 2\n",
      "----- Real caption: \"Still waiting for my AARP card.\"\n",
      "\n",
      "\n",
      "Epoch 4/15\n",
      "243540/243540 [==============================] - 2250s 9ms/step - loss: 1.7995 - accuracy: 0.5097 - val_loss: 2.1666 - val_accuracy: 0.4841\n",
      "\n",
      "Generating caption after epoch 3\n",
      "----- Real caption: \"dreamyüå∏üåø\"\n",
      "IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII\n",
      "\n",
      "Epoch 5/15\n",
      "243540/243540 [==============================] - 2245s 9ms/step - loss: 1.7968 - accuracy: 0.5096 - val_loss: 2.1666 - val_accuracy: 0.4818\n",
      "\n",
      "Generating caption after epoch 4\n",
      "----- Real caption: \"Seeing double!!\"\n",
      "                                                              \n",
      "\n",
      "Epoch 6/15\n",
      "243540/243540 [==============================] - 2259s 9ms/step - loss: 1.7935 - accuracy: 0.5112 - val_loss: 2.1881 - val_accuracy: 0.4854\n",
      "\n",
      "Generating caption after epoch 5\n",
      "----- Real caption: \"Late, but, it was quality night\"\n",
      "                                                              \n",
      "\n",
      "Epoch 7/15\n",
      "243540/243540 [==============================] - 2251s 9ms/step - loss: 1.7886 - accuracy: 0.5122 - val_loss: 2.1815 - val_accuracy: 0.4825\n",
      "\n",
      "Generating caption after epoch 6\n",
      "----- Real caption: \"The rough life ft. Heidinor\"\n",
      "ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss\n",
      "\n",
      "Epoch 8/15\n",
      "243540/243540 [==============================] - 2246s 9ms/step - loss: 1.7857 - accuracy: 0.5129 - val_loss: 2.2069 - val_accuracy: 0.4817\n",
      "\n",
      "Generating caption after epoch 7\n",
      "----- Real caption: \"It‚Äôs comeback szn Juicy tracksuits will rise again in 2018üéÄüíï\"\n",
      "                                                              \n",
      "\n",
      "Epoch 9/15\n",
      "243540/243540 [==============================] - 2248s 9ms/step - loss: 1.7860 - accuracy: 0.5145 - val_loss: 2.2172 - val_accuracy: 0.4852\n",
      "\n",
      "Generating caption after epoch 8\n",
      "----- Real caption: \"Mom beat me to it\"\n",
      "                                                              \n",
      "\n",
      "Epoch 10/15\n",
      "243540/243540 [==============================] - 2257s 9ms/step - loss: 1.7836 - accuracy: 0.5138 - val_loss: 2.2343 - val_accuracy: 0.4868\n",
      "\n",
      "Generating caption after epoch 9\n",
      "----- Real caption: \"Peek-a-boo\"\n",
      "                                                              \n",
      "\n",
      "Epoch 11/15\n",
      "243540/243540 [==============================] - 2249s 9ms/step - loss: 1.7819 - accuracy: 0.5146 - val_loss: 2.2194 - val_accuracy: 0.4810\n",
      "\n",
      "Generating caption after epoch 10\n",
      "----- Real caption: \"üåüüíöüíóHACKEDüéÅüíó\"\n",
      "\n",
      "\n",
      "Epoch 12/15\n",
      "243540/243540 [==============================] - 2255s 9ms/step - loss: 1.7854 - accuracy: 0.5157 - val_loss: 2.2292 - val_accuracy: 0.4853\n",
      "\n",
      "Generating caption after epoch 11\n",
      "----- Real caption: \"Missing these guys right now!\"\n",
      "\n",
      "\n",
      "Epoch 13/15\n",
      "243540/243540 [==============================] - 2304s 9ms/step - loss: 1.7823 - accuracy: 0.5150 - val_loss: 2.2340 - val_accuracy: 0.4840\n",
      "\n",
      "Generating caption after epoch 12\n",
      "----- Real caption: \"Moo if you're excited for homecoming. #donthaveacow\"\n",
      "                                                              \n",
      "\n",
      "Epoch 14/15\n",
      "243540/243540 [==============================] - 2287s 9ms/step - loss: 1.7851 - accuracy: 0.5147 - val_loss: 2.2296 - val_accuracy: 0.4862\n",
      "\n",
      "Generating caption after epoch 13\n",
      "----- Real caption: \"Just two friends monkeyin' around üêíüêí\"\n",
      "                                                              \n",
      "\n",
      "Epoch 15/15\n",
      "243540/243540 [==============================] - 2249s 9ms/step - loss: 1.7862 - accuracy: 0.5144 - val_loss: 2.2345 - val_accuracy: 0.4847\n",
      "\n",
      "Generating caption after epoch 14\n",
      "----- Real caption: \"üåû+üåä=üòôüòô\"\n",
      "tttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttt\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x63e57f940>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train on 80% of the data, test on 20%\n",
    "amt_data = int(len(X1) * 4 / 5)\n",
    "\n",
    "# Create training and testing vectors\n",
    "X1train = np.array(X1[:amt_data])\n",
    "X1test = np.array(X1[amt_data:])\n",
    "\n",
    "X2train = np.array(X2[:amt_data])\n",
    "X2test = np.array(X2[amt_data:])\n",
    "\n",
    "ytrain = np.array(y[:amt_data])\n",
    "ytest = np.array(y[amt_data:])\n",
    "\n",
    "# Fit the model\n",
    "model.fit([X1train, X2train], ytrain, epochs=15, verbose=1, validation_data=([X1test, X2test], ytest), callbacks=[print_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3155\n",
      "Real:\n",
      "\tsup dog\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "6545\n",
      "Real:\n",
      "\tConociendo a personas increibles,caminando 18 km hoy.\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "6219\n",
      "Real:\n",
      "\tsofabulus\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "8987\n",
      "Real:\n",
      "\tLove my girls üë©‚Äçüëß‚Äçüëß @soph512\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "5783\n",
      "Real:\n",
      "\tMy new friend, Richard Parker.\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "6376\n",
      "Real:\n",
      "\tSomeone really loves Twitter. @joeybrunk\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "1293\n",
      "Real:\n",
      "\tWednesday's are for me and my friend Paige\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "1840\n",
      "Real:\n",
      "\tüè† yayy\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "4544\n",
      "Real:\n",
      "\tHalloween was a knock out üí•\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "4056\n",
      "Real:\n",
      "\tMy people ‚ù§Ô∏è (don‚Äôt mind my awkward hand)\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "1490\n",
      "Real:\n",
      "\tMomma-son brunch!\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "3953\n",
      "Real:\n",
      "\t‚Äúweird flex, but ok‚Äù -ranchel nerf\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "8875\n",
      "Real:\n",
      "\tTrying to wave down our helicopter rescue üöÅ\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "5593\n",
      "Real:\n",
      "\tgals, gals, gals\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "5993\n",
      "Real:\n",
      "\t....see you later......in your nightmares......\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "2735\n",
      "Real:\n",
      "\tI spy @ben_barna in #thebigpinkmuff üåä\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "4099\n",
      "Real:\n",
      "\tEveryone Playing flappy bird\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "6959\n",
      "Real:\n",
      "\tOpened up a coffee shop in Boston. We're hiring.\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "7460\n",
      "Real:\n",
      "\tProm pics fall 2017!!\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "7526\n",
      "Real:\n",
      "\tFrom 503 to 305 real quick\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "9641\n",
      "Real:\n",
      "\tscoot scoot bitch\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "7357\n",
      "Real:\n",
      "\toh my Gaud√≠ its ya fave cousins\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "4708\n",
      "Real:\n",
      "\t6,000 miles away yet still representing ChiüáÆüá±üåç\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "5200\n",
      "Real:\n",
      "\tüìç\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "4598\n",
      "Real:\n",
      "\tNot even santa can save me from finals\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "9413\n",
      "Real:\n",
      "\tüñ§ ya\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "4324\n",
      "Real:\n",
      "\tAlmost roomies‚ú®#moveovergabby\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "2161\n",
      "Real:\n",
      "\tNeed someone to look at me the way I look at bruno üòç #puppy\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "2750\n",
      "Real:\n",
      "\tHappy Holidays from 1/23 of spring fair!üéÖüèª‚òÉÔ∏è‚ùÑÔ∏è\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "945\n",
      "Real:\n",
      "\tMiss it like crazyüò©\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "7177\n",
      "Real:\n",
      "\t?\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "2648\n",
      "Real:\n",
      "\tChina actually has umbrella plants lol\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "26\n",
      "Real:\n",
      "\tCheers to four years of fun with my best friends!\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "9718\n",
      "Real:\n",
      "\tLove this peanut head\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "3833\n",
      "Real:\n",
      "\tThings could be stranger but I don't know how\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "8299\n",
      "Real:\n",
      "\tcan‚Äôt afford shit but at least i got some pics : ]\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "4634\n",
      "Real:\n",
      "\tbordeaux stains on the dress, i mean business\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "2215\n",
      "Real:\n",
      "\tLike where‚Äôs waldo but it‚Äôs really easy\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "2390\n",
      "Real:\n",
      "\tIt was a Noah and Clara kind of night\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "4868\n",
      "Real:\n",
      "\t3 eggs in the city\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "924\n",
      "Real:\n",
      "\tbig ball. big smile. BIG fan of cope.\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "3198\n",
      "Real:\n",
      "\tJews love Christmas üéÑ#happyholidays\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "6329\n",
      "Real:\n",
      "\tfabulous '50s meets roaring '20s üíã\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "6400\n",
      "Real:\n",
      "\tSmiling through the pain ü§ï\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "788\n",
      "Real:\n",
      "\t'And in the morning, I'm making waffles!' -donkey\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "8204\n",
      "Real:\n",
      "\tThe Time Has Come. Vote ≈†ariƒá 2015\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "2368\n",
      "Real:\n",
      "\t2 monks and a reincarnated cockroach\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "2998\n",
      "Real:\n",
      "\ttoo bright you‚Äôre gonna need some shadesüï∂üíìüòù\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "3149\n",
      "Real:\n",
      "\tNew year, same boss #Eche\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "537\n",
      "Real:\n",
      "\tcontent‚òïÔ∏è\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "590\n",
      "Real:\n",
      "\tüôá\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "65\n",
      "Real:\n",
      "\tIndian food in London is second to naan.\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "7581\n",
      "Real:\n",
      "\tsuburban blues üîπüî∑\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "7142\n",
      "Real:\n",
      "\tIt‚Äôs coooooollldddddddd ü•∂ü•∂ü•∂ü•∂\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "4698\n",
      "Real:\n",
      "\twhoops another gov ball pic ¬Ø\\_(„ÉÑ)_/¬Ø\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "3959\n",
      "Real:\n",
      "\tScallop & Orange Ceviche\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "2551\n",
      "Real:\n",
      "\tmy favorite people in my favorite place !! #babybuffs\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "5190\n",
      "Real:\n",
      "\tMoved to dc and of course she followed\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "9391\n",
      "Real:\n",
      "\tFamily #4sal\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "5470\n",
      "Real:\n",
      "\tMidnight meditation @sahil_veeramoney #midnight #swim #monks\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "1831\n",
      "Real:\n",
      "\tI wished that we didn't have to leave #iceland #sb2k16\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "8333\n",
      "Real:\n",
      "\tbeautiful night in Bostonüåù\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "9708\n",
      "Real:\n",
      "\tHopkins baes\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "8929\n",
      "Real:\n",
      "\tMissed the Vein more than Zoe\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "8834\n",
      "Real:\n",
      "\tüèÉ R‚ù§N üèÉ\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "7449\n",
      "Real:\n",
      "\tThe fall colors are gourdgeous üçÇ\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "8261\n",
      "Real:\n",
      "\tThe 90s called, they want their denim back ‚òéÔ∏èüëñ\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "4223\n",
      "Real:\n",
      "\tTfw you live for the applause plause üì∏: @livilovie\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "8893\n",
      "Real:\n",
      "\tI promise I'll shave yours soon h17bd ‚ù§Ô∏è‚ù§Ô∏è\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "8667\n",
      "Real:\n",
      "\tüôà peekaboo!\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "8820\n",
      "Real:\n",
      "\tAwesome show at The Wynn Las Vegas #lereve\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "2873\n",
      "Real:\n",
      "\tlove you like you love wings. happy birthday syd!!\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "9156\n",
      "Real:\n",
      "\tsmart & cute\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "8656\n",
      "Real:\n",
      "\tMeet us in Seattle @harrystyles\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "3659\n",
      "Real:\n",
      "\t‚ù§Ô∏èÔ∏èüá±üáª‚ù§Ô∏èÔ∏è\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "8543\n",
      "Real:\n",
      "\tBeautiful place\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "9461\n",
      "Real:\n",
      "\tSanta's Little HelpersüéÖüèª\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "556\n",
      "Real:\n",
      "\tIt's (Bos)ton of fun!!\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "1098\n",
      "Real:\n",
      "\tTrying to clock in as many adventures as possible\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "1236\n",
      "Real:\n",
      "\taspiring third base coach and her bants #tcsb\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "7496\n",
      "Real:\n",
      "\tReady to move here at any time. #nofilter\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "2027\n",
      "Real:\n",
      "\tMy favorite brother\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "5306\n",
      "Real:\n",
      "\tYours truly <3\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "1006\n",
      "Real:\n",
      "\t#montreal #nofilter\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "2942\n",
      "Real:\n",
      "\tmy legs feel like jello\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "9756\n",
      "Real:\n",
      "\tGrind never stops for GMI üò§üí™üèæüí™üèº\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "106\n",
      "Real:\n",
      "\tYesterday mornings sunrise from the Gorge üåÖ\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "2105\n",
      "Real:\n",
      "\tP(l)(r)aying among fallen giants.. In socksüòµ\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "2\n",
      "Real:\n",
      "\t‚Ä¢just beachy‚Ä¢ (ft the remnants of my henna)\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "9253\n",
      "Real:\n",
      "\tSalt & pepper bitch\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "6540\n",
      "Real:\n",
      "\t#loextra√±omucho #loamo\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "8132\n",
      "Real:\n",
      "\tThere are two kinds of people\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "5657\n",
      "Real:\n",
      "\tsparkle summit 2k19: the year of the sparkle stamps\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "1710\n",
      "Real:\n",
      "\tAriana Grande with my favorite rats üòª\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "2392\n",
      "Real:\n",
      "\tCape May #wuht\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "3669\n",
      "Real:\n",
      "\tRoy Molloy in action. #respect\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "5224\n",
      "Real:\n",
      "\tThe perfect fit\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "6792\n",
      "Real:\n",
      "\tFalling or flying? I say both\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "3365\n",
      "Real:\n",
      "\tHiking adventures in freezing water with this lovely üëå\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n",
      "7523\n",
      "Real:\n",
      "\tWake me up when I‚Äôm famous\n",
      "Predicted:\n",
      "\tThe world with the best weekend #thetan\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test fitted model on 100 random images\n",
    "for x in range(100):\n",
    "    i = randint(0, num_images)\n",
    "    print(i)\n",
    "    print(\"Real:\")\n",
    "    print(\"\\t\" + captions[i])\n",
    "    pred_encoded = [start]\n",
    "    f = np.array(features[i])\n",
    "    yhat = 0\n",
    "    pred_capt = \"\"\n",
    "    \n",
    "    # Loop to continue feeding generated string in until we hit the stop sequence\n",
    "    while yhat != stop and len(pred_capt) < 100:\n",
    "        pred_pad = np.array(pad_sequences([pred_encoded], max_cap))\n",
    "        yhat = np.argmax(model.predict([f, pred_pad]))\n",
    "        if yhat == start or yhat == stop:\n",
    "            break\n",
    "        try:\n",
    "            pred_encoded.append(yhat)\n",
    "            pred_capt += to_letter(yhat)\n",
    "        except:\n",
    "            print(\"OH NO\")\n",
    "    print(\"Predicted:\")\n",
    "    print(\"\\t\" + pred_capt)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
